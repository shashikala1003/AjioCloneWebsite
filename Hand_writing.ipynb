{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashikala1003/AjioCloneWebsite/blob/main/Hand_writing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/handwriting_data.zip'"
      ],
      "metadata": {
        "id": "n4dhgGoS9HqN",
        "outputId": "0771016a-0d0a-4e75-eedd-51bd7cdc0503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/handwriting_data.zip, /content/drive/MyDrive/handwriting_data.zip.zip or /content/drive/MyDrive/handwriting_data.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from os import walk\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.measure import find_contours\n",
        "from skimage.morphology import binary_dilation\n",
        "from sklearn.svm import SVC\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#https://www.kaggle.com/datasets/nibinv23/iam-handwriting-word-database"
      ],
      "metadata": {
        "id": "DcRJ_bzD5knZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L8DuuR4XUJu",
        "outputId": "0f165b12-fe8c-4bc8-e510-d3f2c35c36fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m972.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AVAILABLE_WRITERS = 672\n",
        "RESULTS_FILE = 'results.txt'\n",
        "TIME_FILE = 'time.txt'\n",
        "OVERLAPPING_METHOD = 0\n",
        "LINES_METHOD = 1\n",
        "SUPPORT_VECTOR_CLASSIFIER = 0\n",
        "NEURAL_NETWORK_CLASSIFIER = 1\n",
        "HISTOGRAM_BINS = 256\n",
        "NN_LEARNING_RATE = 0.003\n",
        "NN_WEIGHT_DECAY = 0.01\n",
        "NN_DROPOUT = 0.25\n",
        "NN_EPOCHS = 200\n",
        "NN_BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "j-DO4QGk5klf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def show_images(images, titles=None):\n",
        "    n_ims = len(images)\n",
        "    if titles is None:\n",
        "        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n",
        "    fig = plt.figure()\n",
        "    n = 1\n",
        "    for image, title in zip(images, titles):\n",
        "        a = fig.add_subplot(1, n_ims, n)\n",
        "        if image.ndim == 2:\n",
        "            plt.gray()\n",
        "        plt.imshow(image)\n",
        "        a.set_title(title)\n",
        "        n += 1\n",
        "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lXuh9CbM5kjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_image(img, feature_extraction_method=OVERLAPPING_METHOD):\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        img_copy = img.copy()\n",
        "        if len(img.shape) > 2:\n",
        "            img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
        "        img_copy = cv2.medianBlur(img_copy, 5)\n",
        "        img_copy = cv2.threshold(img_copy, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n",
        "        img_copy = img_copy[min_vertical:max_vertical]\n",
        "        return img_copy\n",
        "\n",
        "    if feature_extraction_method == LINES_METHOD:\n",
        "        img_copy = img.copy()\n",
        "        if len(img.shape) > 2:\n",
        "            grayscale_img = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            grayscale_img = img.copy()\n",
        "        img_copy = cv2.threshold(grayscale_img, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n",
        "        img_copy = img_copy[min_vertical:max_vertical]\n",
        "        grayscale_img = grayscale_img[min_vertical:max_vertical]\n",
        "        filter_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
        "        img_copy_sharpened = cv2.filter2D(img_copy, -1, filter_kernel)\n",
        "        return img_copy_sharpened, grayscale_img"
      ],
      "metadata": {
        "id": "AcCnYCny5kgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus_boundaries(img):\n",
        "    crop = []\n",
        "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (100, 1))\n",
        "    detect_horizontal = cv2.morphologyEx(img, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
        "    contours = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "    prev = -1\n",
        "    for i, c in enumerate(contours):\n",
        "        if np.abs(prev - int(c[0][0][1])) > 800 or prev == -1:\n",
        "            crop.append(int(c[0][0][1]))\n",
        "            prev = int(c[0][0][1])\n",
        "    crop.sort()\n",
        "    max_vertical = crop[1] - 20\n",
        "    min_vertical = crop[0] + 20\n",
        "    return min_vertical, max_vertical"
      ],
      "metadata": {
        "id": "qHOBh1iY5kar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_image(img, num, grayscale_img=None):\n",
        "    if grayscale_img is not None:\n",
        "        grayscale_images = []\n",
        "        img_copy = np.copy(img)\n",
        "        kernel = np.ones((1, num))\n",
        "        img_copy = binary_dilation(img_copy, kernel)\n",
        "        bounding_boxes = find_contours(img_copy, 0.8)\n",
        "        for box in bounding_boxes:\n",
        "            x_min = int(np.min(box[:, 1]))\n",
        "            x_max = int(np.max(box[:, 1]))\n",
        "            y_min = int(np.min(box[:, 0]))\n",
        "            y_max = int(np.max(box[:, 0]))\n",
        "            if (y_max - y_min) > 50 and (x_max - x_min) > 50:\n",
        "                grayscale_images.append(grayscale_img[y_min:y_max, x_min:x_max])\n",
        "        return grayscale_images\n",
        "    images = []\n",
        "    img_copy = np.copy(img)\n",
        "    kernel = np.ones((1, num))\n",
        "    img_copy = binary_dilation(img_copy, kernel)\n",
        "    bounding_boxes = find_contours(img_copy, 0.8)\n",
        "    for box in bounding_boxes:\n",
        "        x_min = int(np.min(box[:, 1]))\n",
        "        x_max = int(np.max(box[:, 1]))\n",
        "        y_min = int(np.min(box[:, 0]))\n",
        "        y_max = int(np.max(box[:, 0]))\n",
        "        if (y_max - y_min) > 10 and (x_max - x_min) > 10:\n",
        "            images.append(img[y_min:y_max, x_min:x_max])\n",
        "    return images"
      ],
      "metadata": {
        "id": "sB3bYEgI5kXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlap_words(words, avg_height):\n",
        "    overlapped_img = np.zeros((3600, 320))\n",
        "    index_i = 0\n",
        "    index_j = 0\n",
        "    max_height = 0\n",
        "    for word in words:\n",
        "        if word.shape[1] + index_j > overlapped_img.shape[1]:\n",
        "            max_height = 0\n",
        "            index_j = 0\n",
        "            index_i += int(avg_height // 2)\n",
        "        if word.shape[1] < overlapped_img.shape[1] and word.shape[0] < overlapped_img.shape[0]:\n",
        "            indices = np.copy(overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]])\n",
        "            indices = np.maximum(indices, word)\n",
        "            overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]] = indices\n",
        "            index_j += word.shape[1]\n",
        "            if max_height < word.shape[0]:\n",
        "                max_height = word.shape[0]\n",
        "    overlapped_img = overlapped_img[:index_i + int(avg_height // 2), :]\n",
        "    return overlapped_img"
      ],
      "metadata": {
        "id": "qYjlpotN56-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textures(image):\n",
        "    index_i = 0\n",
        "    index_j = 0\n",
        "    texture_size = 100\n",
        "    textures = []\n",
        "    while index_i + texture_size < image.shape[0]:\n",
        "        if index_j + texture_size > image.shape[1]:\n",
        "            index_j = 0\n",
        "            index_i += texture_size\n",
        "        textures.append(np.copy(image[index_i: index_i + texture_size, index_j: index_j + texture_size]))\n",
        "        index_j += texture_size\n",
        "    return textures"
      ],
      "metadata": {
        "id": "NYGqGcrS567W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generator(features, labels, feature_extraction_method=OVERLAPPING_METHOD,\n",
        "                    classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n",
        "    histograms = []\n",
        "\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        for texture_array in features:\n",
        "            for texture in texture_array:\n",
        "                lbp = local_binary_pattern(texture, 8, 3, 'default')\n",
        "                histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "                histograms.append(histogram)\n",
        "\n",
        "    elif feature_extraction_method == LINES_METHOD:\n",
        "        for line in features:\n",
        "            lbp = local_binary_pattern(line, 8, 3, 'default')\n",
        "            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "            histograms.append(histogram)\n",
        "\n",
        "    if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n",
        "        model = SVC(kernel='linear')\n",
        "        model.fit(histograms, labels)\n",
        "        return model\n",
        "\n",
        "    if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n",
        "        model = nn.Sequential(nn.Linear(HISTOGRAM_BINS, 128),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(p=NN_DROPOUT),\n",
        "                              nn.Linear(128, 64),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(p=NN_DROPOUT),\n",
        "                              nn.Linear(64, 3))\n",
        "        model.to(DEVICE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adamax(model.parameters(), lr=NN_LEARNING_RATE, weight_decay=NN_WEIGHT_DECAY)\n",
        "        inputs = torch.Tensor(histograms)\n",
        "        labels = torch.tensor(labels, dtype=torch.long) - 1\n",
        "        dataset = TensorDataset(inputs, labels)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=NN_BATCH_SIZE, shuffle=True)\n",
        "        for epoch in range(NN_EPOCHS):\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                output = model(inputs)\n",
        "                loss = criterion(output, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model"
      ],
      "metadata": {
        "id": "WzHNLSPg5643"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_image, feature_extraction_method=OVERLAPPING_METHOD, classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        img = preprocess_image(test_image)\n",
        "        words = segment_image(img, 3)\n",
        "        avg_height = 0\n",
        "        for word in words:\n",
        "            avg_height += word.shape[0] / len(words)\n",
        "        overlapped_img = overlap_words(words, avg_height)\n",
        "        textures = get_textures(overlapped_img)\n",
        "        prediction = np.zeros(4)\n",
        "        for texture in textures:\n",
        "            lbp = local_binary_pattern(texture, 8, 3, 'default')\n",
        "            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n",
        "                prediction[model.predict([histogram])] += 1\n",
        "            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    histogram = torch.Tensor(histogram)\n",
        "                    probabilities = F.softmax(model.forward(histogram), dim=0)\n",
        "                    _, top_class = probabilities.topk(1)\n",
        "                    prediction[top_class + 1] += 1\n",
        "        return np.argmax(prediction)\n",
        "\n",
        "    if feature_extraction_method == LINES_METHOD:\n",
        "        img, grayscale_img = preprocess_image(test_image, feature_extraction_method)\n",
        "        grayscale_lines = segment_image(img, 100, grayscale_img)\n",
        "        prediction = np.zeros(4)\n",
        "        for line in grayscale_lines:\n",
        "            lbp = local_binary_pattern(line, 8, 3, 'default')\n",
        "            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n",
        "                prediction[model.predict([histogram])] += 1\n",
        "            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    histogram = torch.Tensor(histogram)\n",
        "                    probabilities = F.softmax(model.forward(histogram), dim=0)\n",
        "                    _, top_class = probabilities.topk(1)\n",
        "                    prediction[top_class + 1] += 1\n",
        "        return np.argmax(prediction)"
      ],
      "metadata": {
        "id": "nSRf77YB6O_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_random_images(root):\n",
        "    images = []\n",
        "    labels = []\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "    for i in range(3):\n",
        "        found_images = False\n",
        "        while not found_images:\n",
        "            images_path = root\n",
        "            random_writer = random.randrange(AVAILABLE_WRITERS)\n",
        "            if random_writer < 10:\n",
        "                random_writer = \"00\" + str(random_writer)\n",
        "            elif random_writer < 100:\n",
        "                random_writer = \"0\" + str(random_writer)\n",
        "            images_path = os.path.join(images_path, str(random_writer))\n",
        "            if not os.path.isdir(images_path):\n",
        "                continue\n",
        "            _, _, filenames = next(walk(images_path))\n",
        "            if len(filenames) <= 2 and i == 2 and len(test_images) == 0:\n",
        "                continue\n",
        "            if len(filenames) >= 2:\n",
        "                found_images = True\n",
        "                chosen_filenames = []\n",
        "                for j in range(2):\n",
        "                    random_filename = random.choice(filenames)\n",
        "                    while random_filename in chosen_filenames:\n",
        "                        random_filename = random.choice(filenames)\n",
        "                    chosen_filenames.append(random_filename)\n",
        "                    images.append(cv2.imread(os.path.join(images_path, random_filename)))\n",
        "                    labels.append(i + 1)\n",
        "                if len(filenames) >= 3:\n",
        "                    random_filename = random.choice(filenames)\n",
        "                    while random_filename in chosen_filenames:\n",
        "                        random_filename = random.choice(filenames)\n",
        "                    chosen_filenames.append(random_filename)\n",
        "                    test_images.append(cv2.imread(os.path.join(images_path, random_filename)))\n",
        "                    test_labels.append(i + 1)\n",
        "    test_choice = random.randint(0, len(test_images) - 1)\n",
        "    test_image = test_images[test_choice]\n",
        "    test_label = test_labels[test_choice]\n",
        "    return images, labels, test_image, test_label"
      ],
      "metadata": {
        "id": "mr6-t6aP6O9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twZp3Bvc5eNN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def extract_features(images, labels, feature_extraction_method=OVERLAPPING_METHOD):\n",
        "    if feature_extraction_method == LINES_METHOD:\n",
        "        lines_labels = []\n",
        "        lines = []\n",
        "        for image, label in zip(images, labels):\n",
        "            image, grayscale_image = preprocess_image(image, feature_extraction_method)\n",
        "            grayscale_lines = segment_image(image, 100, grayscale_image)\n",
        "            for line in grayscale_lines:\n",
        "                lines.append(line)\n",
        "                lines_labels.append(label)\n",
        "        return lines, lines_labels\n",
        "\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        textures = []\n",
        "        textures_labels = []\n",
        "        for image, label in zip(images, labels):\n",
        "            image = preprocess_image(image)\n",
        "            words = segment_image(image, 3)\n",
        "            avg_height = 0\n",
        "            for word in words:\n",
        "                avg_height += word.shape[0] / len(words)\n",
        "            overlapped_img = overlap_words(words, avg_height)\n",
        "            new_textures = get_textures(overlapped_img)\n",
        "            textures.append(new_textures)\n",
        "            for j in range(len(new_textures)):\n",
        "                textures_labels.append(label)\n",
        "        return textures, textures_labels\n",
        "epochs = 100\n",
        "root='../input/iam-handwritten-forms-dataset/data'\n",
        "feature_extraction_method=OVERLAPPING_METHOD\n",
        "classifier_type=SUPPORT_VECTOR_CLASSIFIER\n",
        "correct_predictions = 0\n",
        "total_execution_time = 0\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    images, labels, test_image, test_label = read_random_images(root)\n",
        "    start_time = time.time()\n",
        "    features, features_labels = extract_features(images, labels, feature_extraction_method)\n",
        "    model = model_generator(features, features_labels, feature_extraction_method, classifier_type)\n",
        "    prediction = predict(model, test_image, feature_extraction_method, classifier_type)\n",
        "    execution_time = time.time() - start_time\n",
        "    total_execution_time += execution_time\n",
        "    if prediction == test_label:\n",
        "        correct_predictions += 1\n",
        "    print(\"Epoch #{} | Execution time {} seconds | Model accuracy {}\".format(epoch + 1, round(execution_time, 2), round((correct_predictions / (epoch + 1)) * 100, 2)))\n",
        "\n",
        "    # Save the trained model after the last epoch\n",
        "    if epoch == epochs - 1:\n",
        "        trained_model = model\n",
        "\n",
        "# Check if the model has been trained\n",
        "if trained_model is not None:\n",
        "    # Save the trained model\n",
        "    model_filepath = 'trained_model.pth'\n",
        "    save_model(trained_model, model_filepath)\n",
        "\n",
        "print(\"Model accuracy = {}% using {} sample tests.\".format((correct_predictions / epochs) * 100, epochs))\n",
        "print(\"Total execution time = {} using {} sample tests.\".format(round(total_execution_time, 2), epochs))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Define your encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # Add more layers as needed\n",
        "        )\n",
        "        # Define your decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid(),\n",
        "            # Add more layers as needed\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Prepare your dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize images to a uniform size if needed\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(root='/content/data', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize the autoencoder\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "# Train the autoencoder\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        img, _ = data\n",
        "        optimizer.zero_grad()\n",
        "        recon = autoencoder(img)\n",
        "        loss = criterion(recon, img)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Extract features using the encoder\n",
        "features = []\n",
        "labels = []\n",
        "for data in train_loader:\n",
        "    img, label = data\n",
        "    with torch.no_grad():\n",
        "        encoded_img = autoencoder.encoder(img)\n",
        "    features.append(encoded_img)\n",
        "    labels.append(label)\n",
        "\n",
        "# Flatten and concatenate features\n",
        "features = torch.cat([feat.view(feat.size(0), -1) for feat in features], dim=0)\n",
        "labels = torch.cat(labels, dim=0)"
      ],
      "metadata": {
        "id": "y92QDgyy-zFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188322cf-6cf9-48cf-fd25-1986aa53692f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0374\n",
            "Epoch [2/10], Loss: 0.0037\n",
            "Epoch [3/10], Loss: 0.0037\n",
            "Epoch [4/10], Loss: 0.0039\n",
            "Epoch [5/10], Loss: 0.0029\n",
            "Epoch [6/10], Loss: 0.0032\n",
            "Epoch [7/10], Loss: 0.0034\n",
            "Epoch [8/10], Loss: 0.0029\n",
            "Epoch [9/10], Loss: 0.0029\n",
            "Epoch [10/10], Loss: 0.0020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2HA8W_989X_",
        "outputId": "29c41577-b0da-4a25-ce7f-fe5a158f46e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}